{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e11d43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import time\n",
    "import tracemalloc\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from math import sqrt\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "import tracemalloc\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# get the start time\n",
    "st = time.time()\n",
    "tracemalloc.start()\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('restaurant.csv')\n",
    "\n",
    "# Prepare the inputs and labels\n",
    "texts = df['review'].values\n",
    "labels = df['label'].values\n",
    "\n",
    "# Load the RoBERTa tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "model = AutoModel.from_pretrained(\"roberta-large\")\n",
    "\n",
    "# Extract features using RoBERTa\n",
    "inputs = np.zeros((len(texts), 1024))\n",
    "for i, text in enumerate(texts):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids).last_hidden_state\n",
    "    inputs[i, :] = last_hidden_states[0, 0, :].numpy()\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(inputs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the classifiers for the voting ensemble\n",
    "svc = SVC(kernel='linear', C=1.0, probability=True)\n",
    "ada = AdaBoostClassifier(n_estimators=100)\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Define the voting ensemble classifier\n",
    "voting_clf = VotingClassifier(estimators=[('svc', svc), ('ada', ada), ('rf', rf)], voting='soft')\n",
    "\n",
    "# Perform the k-fold cross-validation to tune hyperparameters\n",
    "for train_index, val_index in kf.split(X_train_val):\n",
    "    X_train, X_val = X_train_val[train_index], X_train_val[val_index]\n",
    "    y_train, y_val = y_train_val[train_index], y_train_val[val_index]\n",
    "    \n",
    "    # Train the voting ensemble classifier\n",
    "    voting_clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the classifier on the validation set\n",
    "    val_pred = voting_clf.predict(X_val)\n",
    "    val_acc = accuracy_score(y_val, val_pred)\n",
    "    print(\"Validation accuracy:\", val_acc)\n",
    "    \n",
    "    # Reset the classifiers\n",
    "    svc = SVC(kernel='linear', C=1.0, probability=True)\n",
    "    ada = AdaBoostClassifier(n_estimators=100)\n",
    "    rf = RandomForestClassifier(n_estimators=100)\n",
    "    voting_clf = VotingClassifier(estimators=[('svc', svc), ('ada', ada), ('rf', rf)], voting='soft')\n",
    "\n",
    "# Evaluate the voting ensemble classifier on the test set\n",
    "voting_clf.fit(X_train_val, y_train_val)\n",
    "test_pred = voting_clf.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "print(\"Test accuracy for Voting:\", test_acc)\n",
    "rms= sqrt(mean_squared_error(y_test, test_pred))\n",
    "print(\"RMSE Error is: \" + str(rms))\n",
    "\n",
    "# Train a BaggingClassifier using Logistic Regression as the base estimator\n",
    "clf = BaggingClassifier(base_estimator=LogReg_clf, n_estimators=100, random_state=42)\n",
    "clf.fit(X_train_val, y_train_val)\n",
    "test_pred = voting_clf.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "print(\"Test accuracy for Bag LR:\", test_acc)\n",
    "rms= sqrt(mean_squared_error(y_test, test_pred))\n",
    "print(\"RMSE Error is: \" + str(rms))\n",
    "\n",
    "# Train a BaggingClassifier using RF as the base estimator\n",
    "clf = BaggingClassifier(base_estimator=rf, n_estimators=100, random_state=42)\n",
    "clf.fit(X_train_val, y_train_val)\n",
    "test_pred = voting_clf.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "print(\"Test accuracy for Bag RF:\", test_acc)\n",
    "rms= sqrt(mean_squared_error(y_test, test_pred))\n",
    "print(\"RMSE Error is: \" + str(rms))\n",
    "\n",
    "# Train a BaggingClassifier using DTree as the base estimator\n",
    "clf = BaggingClassifier(base_estimator=DTree_clf, n_estimators=100, random_state=42)\n",
    "clf.fit(X_train_val, y_train_val)\n",
    "test_pred = voting_clf.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "print(\"Test accuracy for Bag DTree:\", test_acc)\n",
    "rms= sqrt(mean_squared_error(y_test, test_pred))\n",
    "print(\"RMSE Error is: \" + str(rms))\n",
    "\n",
    "clf = xgb.XGBClassifier(n_estimators=70, learning_rate=0.9)\n",
    "clf.fit(X_train_val, y_train_val)\n",
    "test_pred = voting_clf.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "print(\"Test accuracy for XBoost:\", test_acc)\n",
    "rms= sqrt(mean_squared_error(y_test, test_pred))\n",
    "print(\"RMSE Error is: \" + str(rms))\n",
    "\n",
    "\n",
    "# get the end time\n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "print('Execution time:', elapsed_time, 'seconds')\n",
    "\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "# Stop tracemalloc\n",
    "tracemalloc.stop()\n",
    "print(\"Current memory usage is\", current / (1024 * 1024), \"MB; Peak was\", peak / (1024 * 1024), \"MB\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
