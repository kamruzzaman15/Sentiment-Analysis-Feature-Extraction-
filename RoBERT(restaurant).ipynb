{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e0124e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82e9c6e56c341e99dfb3b2db0f99b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c139080817c4f3a8e405ad338d87480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ad53f9e9da48f09d946ca2bd037e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df64fb17ab984bc7ad47ea7812dc97c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f66e2bf72b944d0aba90a44d4d17366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('restaurant.csv')\n",
    "\n",
    "# Prepare the inputs and labels\n",
    "texts = df['review'].values\n",
    "labels = df['label'].values\n",
    "\n",
    "# Load the RoBERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Extract features using RoBERT\n",
    "inputs = np.zeros((len(texts), 768))\n",
    "for i, text in enumerate(texts):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids).last_hidden_state\n",
    "    inputs[i, :] = last_hidden_states[0, 0, :].numpy()\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_inputs = inputs[:int(0.8 * len(inputs))]\n",
    "train_labels = labels[:int(0.8 * len(labels))]\n",
    "test_inputs = inputs[int(0.8 * len(inputs)):]\n",
    "test_labels = labels[int(0.8 * len(labels)):]\n",
    "\n",
    "# Train a Gradient Boosting Classifier\n",
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(train_inputs, train_labels)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = clf.predict(test_inputs)\n",
    "\n",
    "# Evaluate the model using accuracy\n",
    "acc = accuracy_score(test_labels, predictions)\n",
    "print('Accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef339969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd93d376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2023-02-21 21:43:30.306036: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-21 21:43:31.144925: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-21 21:43:31.145018: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-21 21:43:33.103672: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-21 21:43:33.103861: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-21 21:43:33.103878: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a79898dbaf4516a9db92e491d2fa90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"tf_model.h5\";:   0%|          | 0.00/657M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('restaurant.csv')\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the training set into training and validation sets for hyperparameter tuning\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load the RoBERTa tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta_model = TFRobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "# Tokenize the text data\n",
    "X_train_tokens = tokenizer(X_train.tolist(), padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
    "X_val_tokens = tokenizer(X_val.tolist(), padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
    "X_test_tokens = tokenizer(X_test.tolist(), padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
    "\n",
    "# Extract RoBERTa features from the text data\n",
    "X_train_features = roberta_model(X_train_tokens)['pooler_output']\n",
    "X_val_features = roberta_model(X_val_tokens)['pooler_output']\n",
    "X_test_features = roberta_model(X_test_tokens)['pooler_output']\n",
    "\n",
    "# Tune hyperparameters using k-fold cross-validation\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "svm_params = {'C': [0.1, 1, 10, 100], 'gamma': ['scale', 'auto']}\n",
    "svm_grid = GridSearchCV(SVC(probability=True), svm_params, scoring='accuracy', cv=kfold)\n",
    "svm_grid.fit(X_train_features, y_train)\n",
    "\n",
    "ada_params = {'n_estimators': [50, 100, 200], 'learning_rate': [0.1, 1, 10]}\n",
    "ada_grid = GridSearchCV(AdaBoostClassifier(), ada_params, scoring='accuracy', cv=kfold)\n",
    "ada_grid.fit(X_train_features, y_train)\n",
    "\n",
    "rf_params = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]}\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(), rf_params, scoring='accuracy', cv=kfold)\n",
    "rf_grid.fit(X_train_features, y_train)\n",
    "\n",
    "# Build the voting ensemble classifier with the tuned hyperparameters\n",
    "svm_best = svm_grid.best_estimator_\n",
    "ada_best = ada_grid.best_estimator_\n",
    "rf_best = rf_grid.best_estimator_\n",
    "\n",
    "ensemble = VotingClassifier(estimators=[('svm', svm_best), ('ada', ada_best), ('rf', rf_best)], voting='soft')\n",
    "ensemble.fit(X_train_features, y_train)\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "test_preds = ensemble.predict(X_test_features)\n",
    "test_acc = accuracy_score(y_test, test_preds)\n",
    "print(\"Test accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a5c6824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2023-02-21 22:00:00.450754: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-21 22:00:00.596970: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-21 22:00:00.596990: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-21 22:00:01.527499: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-21 22:00:01.527619: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-21 22:00:01.527631: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(texts):\n\u001b[1;32m     24\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     26\u001b[0m         last_hidden_states \u001b[38;5;241m=\u001b[39m model(input_ids)\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     27\u001b[0m     inputs[i, :] \u001b[38;5;241m=\u001b[39m last_hidden_states[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, :]\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Prepare the inputs and labels\n",
    "texts = df['text'].values\n",
    "labels = df['label'].values\n",
    "\n",
    "# Load the RoBERTa tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Extract features using RoBERTa\n",
    "inputs = np.zeros((len(texts), 768))\n",
    "for i, text in enumerate(texts):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids).last_hidden_state\n",
    "    inputs[i, :] = last_hidden_states[0, 0, :].numpy()\n",
    "\n",
    "# Divide the data into training, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.2, random_state=42)\n",
    "kf = KFold(n_splits=5, random_state=42)\n",
    "\n",
    "# Define the classifiers\n",
    "svc = SVC(kernel='linear')\n",
    "ada = AdaBoostClassifier(n_estimators=200)\n",
    "rf = RandomForestClassifier(n_estimators=200)\n",
    "\n",
    "# Define the voting ensemble classifier\n",
    "clf = VotingClassifier(estimators=[('svc', svc), ('ada', ada), ('rf', rf)], voting='hard')\n",
    "\n",
    "# Define the hyperparameters to be tuned\n",
    "params = {'svc__C': [0.1, 1, 10],\n",
    "          'ada__n_estimators': [100, 200],\n",
    "          'rf__max_depth': [10, 20],\n",
    "          'rf__max_features': ['sqrt', 'log2']}\n",
    "\n",
    "# Tune the hyperparameters using GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=params, cv=kf, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = grid_search.predict(X_test)\n",
    "\n",
    "# Evaluate the model using accuracy\n",
    "acc = accuracy_score(y_test, predictions)\n",
    "print('Accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2f8022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5fb9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, VotingClassifier\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"restaurant.csv\")\n",
    "\n",
    "# Split the data into training, validation, and test sets using k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_val_idx, test_idx = next(kf.split(df))\n",
    "train_idx, val_idx = next(kf.split(df.iloc[train_val_idx]))\n",
    "\n",
    "# Split the data into features and labels\n",
    "X_train_val, y_train_val = df['review'].iloc[train_val_idx], df['label'].iloc[train_val_idx]\n",
    "X_train, y_train = X_train_val.iloc[train_idx], y_train_val.iloc[train_idx]\n",
    "X_val, y_val = X_train_val.iloc[val_idx], y_train_val.iloc[val_idx]\n",
    "X_test, y_test = df['review'].iloc[test_idx], df['label'].iloc[test_idx]\n",
    "\n",
    "# Initialize the RoBERTa tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "# Extract features from the data using the RoBERTa model\n",
    "X_train_features = []\n",
    "for sentence in tqdm(X_train):\n",
    "    input_ids = torch.tensor(tokenizer.encode(sentence)).unsqueeze(0)\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_states = outputs[0].squeeze().detach().numpy()\n",
    "    X_train_features.append(last_hidden_states)\n",
    "X_train_features = np.array(X_train_features)\n",
    "\n",
    "X_val_features = []\n",
    "for sentence in tqdm(X_val):\n",
    "    input_ids = torch.tensor(tokenizer.encode(sentence)).unsqueeze(0)\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_states = outputs[0].squeeze().detach().numpy()\n",
    "    X_val_features.append(last_hidden_states)\n",
    "X_val_features = np.array(X_val_features)\n",
    "\n",
    "X_test_features = []\n",
    "for sentence in tqdm(X_test):\n",
    "    input_ids = torch.tensor(tokenizer.encode(sentence)).unsqueeze(0)\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_states = outputs[0].squeeze().detach().numpy()\n",
    "    X_test_features.append(last_hidden_states)\n",
    "X_test_features = np.array(X_test_features)\n",
    "\n",
    "# Initialize the classifiers\n",
    "svm = SVC(kernel='linear', probability=True)\n",
    "ada = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Combine the classifiers into a voting ensemble\n",
    "clf = VotingClassifier(estimators=[('svm', svm), ('ada', ada), ('rf', rf)], voting='soft')\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train_features, y_train)\n",
    "\n",
    "# Evaluate the classifier on the validation and test sets\n",
    "val_acc = clf.score(X_val_features, y_val)\n",
    "test_acc = clf.score(X_test_features, y_test)\n",
    "\n",
    "print(f\"Validation accuracy: {val_acc}\")\n",
    "print(f\"Test accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c3a2b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06d0d582",
   "metadata": {},
   "source": [
    "# Accuracy 94...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be8ddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.ensemble import VotingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('restaurant.csv')\n",
    "\n",
    "# Prepare the inputs and labels\n",
    "texts = df['review'].values\n",
    "labels = df['label'].values\n",
    "\n",
    "# Load the RoBERTa tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Extract features using RoBERTa\n",
    "inputs = np.zeros((len(texts), 768))\n",
    "for i, text in enumerate(texts):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids).last_hidden_state\n",
    "    inputs[i, :] = last_hidden_states[0, 0, :].numpy()\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the ensemble classifiers\n",
    "svm = SVC(kernel='linear', C=1, probability=True, random_state=42)\n",
    "ada = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Define the voting ensemble classifier\n",
    "voting_clf = VotingClassifier(estimators=[('svm', svm), ('ada', ada), ('rf', rf)], voting='soft')\n",
    "\n",
    "# Train the voting ensemble classifier\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = voting_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model using accuracy\n",
    "acc = accuracy_score(y_test, predictions)\n",
    "print('Accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3e8ec6",
   "metadata": {},
   "source": [
    "# RoBERT Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a07f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.ensemble import VotingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('GrammarandProductReviews[modified].csv')\n",
    "\n",
    "# Prepare the inputs and labels\n",
    "texts = df['review'].values\n",
    "labels = df['positive_review'].values\n",
    "\n",
    "# Load the RoBERTa tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "model = AutoModel.from_pretrained(\"roberta-large\")\n",
    "\n",
    "# Extract features using RoBERTa\n",
    "inputs = np.zeros((len(texts), 1024))\n",
    "for i, text in enumerate(texts):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids).last_hidden_state\n",
    "    inputs[i, :] = last_hidden_states[0, 0, :].numpy()\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the ensemble classifiers\n",
    "svm = SVC(kernel='linear', C=1, probability=True, random_state=42)\n",
    "ada = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Define the voting ensemble classifier\n",
    "voting_clf = VotingClassifier(estimators=[('svm', svm), ('ada', ada), ('rf', rf)], voting='soft')\n",
    "\n",
    "# Train the voting ensemble classifier\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = voting_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model using accuracy\n",
    "acc = accuracy_score(y_test, predictions)\n",
    "print('Accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc886a1",
   "metadata": {},
   "source": [
    "# Looks perfect with 95...accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cbd857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import time\n",
    "import tracemalloc\n",
    "\n",
    "# get the start time\n",
    "st = time.time()\n",
    "tracemalloc.start()\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('restaurant.csv')\n",
    "\n",
    "# Prepare the inputs and labels\n",
    "texts = df['review'].values\n",
    "labels = df['label'].values\n",
    "\n",
    "# Load the RoBERTa tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "model = AutoModel.from_pretrained(\"roberta-large\")\n",
    "\n",
    "# Extract features using RoBERTa\n",
    "inputs = np.zeros((len(texts), 1024))\n",
    "for i, text in enumerate(texts):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids).last_hidden_state\n",
    "    inputs[i, :] = last_hidden_states[0, 0, :].numpy()\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(inputs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the classifiers for the voting ensemble\n",
    "svc = SVC(kernel='linear', C=1.0, probability=True)\n",
    "ada = AdaBoostClassifier(n_estimators=100)\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Define the voting ensemble classifier\n",
    "voting_clf = VotingClassifier(estimators=[('svc', svc), ('ada', ada), ('rf', rf)], voting='soft')\n",
    "\n",
    "# Perform the k-fold cross-validation to tune hyperparameters\n",
    "for train_index, val_index in kf.split(X_train_val):\n",
    "    X_train, X_val = X_train_val[train_index], X_train_val[val_index]\n",
    "    y_train, y_val = y_train_val[train_index], y_train_val[val_index]\n",
    "    \n",
    "    # Train the voting ensemble classifier\n",
    "    voting_clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the classifier on the validation set\n",
    "    val_pred = voting_clf.predict(X_val)\n",
    "    val_acc = accuracy_score(y_val, val_pred)\n",
    "    print(\"Validation accuracy:\", val_acc)\n",
    "    \n",
    "    # Reset the classifiers\n",
    "    svc = SVC(kernel='linear', C=1.0, probability=True)\n",
    "    ada = AdaBoostClassifier(n_estimators=100)\n",
    "    rf = RandomForestClassifier(n_estimators=100)\n",
    "    voting_clf = VotingClassifier(estimators=[('svc', svc), ('ada', ada), ('rf', rf)], voting='soft')\n",
    "\n",
    "# Evaluate the voting ensemble classifier on the test set\n",
    "voting_clf.fit(X_train_val, y_train_val)\n",
    "test_pred = voting_clf.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "\n",
    "\n",
    "# get the end time\n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "print('Execution time:', elapsed_time, 'seconds')\n",
    "\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "# Stop tracemalloc\n",
    "tracemalloc.stop()\n",
    "print(\"Current memory usage is\", current / (1024 * 1024), \"MB; Peak was\", peak / (1024 * 1024), \"MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab142e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d02b442",
   "metadata": {},
   "source": [
    "# Bagged (LR=92.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7023a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.ensemble import VotingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "import torch\n",
    "import time\n",
    "import tracemalloc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# get the start time\n",
    "st = time.time()\n",
    "tracemalloc.start()\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('GrammarandProductReviews[modified].csv')\n",
    "\n",
    "# Prepare the inputs and labels\n",
    "texts = df['review'].values\n",
    "labels = df['positive_review'].values\n",
    "\n",
    "# Load the RoBERTa tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "model = AutoModel.from_pretrained(\"roberta-large\")\n",
    "\n",
    "# Extract features using RoBERTa\n",
    "inputs = np.zeros((len(texts), 1024))\n",
    "for i, text in enumerate(texts):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids).last_hidden_state\n",
    "    inputs[i, :] = last_hidden_states[0, 0, :].numpy()\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the ensemble classifiers\n",
    "svm = SVC(kernel='linear', C=1, probability=True, random_state=42)\n",
    "ada = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "LogReg_clf = LogisticRegression(max_iter=1000)\n",
    "DTree_clf = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "#Bagging Ensemble Method\n",
    "logreg_bagging_model = BaggingClassifier(base_estimator=LogReg_clf, n_estimators=100, random_state=142)\n",
    "dtree_bagging_model = BaggingClassifier(base_estimator=DTree_clf, n_estimators=100, random_state=142)\n",
    "#random_forest = BaggingClassifier(base_estimator=rf, n_estimators=50, random_state=100)\n",
    "#extra_trees = BaggingClassifier(base_estimator=ETree, n_estimators=50, random_state=100)\n",
    "\n",
    "def bagging_ensemble(bag):\n",
    "    k_folds = KFold(n_splits=15)\n",
    "    results = cross_val_score(bag, X_train, y_train, cv=k_folds)\n",
    "    print(results.mean())\n",
    "\n",
    "\n",
    "bagging_ensemble(logreg_bagging_model)\n",
    "bagging_ensemble(dtree_bagging_model)\n",
    "#bagging_ensemble(random_forest)\n",
    "#bagging_ensemble(extra_trees)\n",
    "\n",
    "\n",
    "# get the end time\n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "print('Execution time:', elapsed_time, 'seconds')\n",
    "\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "# Stop tracemalloc\n",
    "tracemalloc.stop()\n",
    "print(\"Current memory usage is\", current / (1024 * 1024), \"MB; Peak was\", peak / (1024 * 1024), \"MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cb5089",
   "metadata": {},
   "source": [
    "# RoBERT Large with product review and voting and bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feb11fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split, KFold,  cross_val_score\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import time\n",
    "import tracemalloc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# get the start time\n",
    "st = time.time()\n",
    "tracemalloc.start()\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('GrammarandProductReviews[modified].csv')\n",
    "\n",
    "# Prepare the inputs and labels\n",
    "texts = df['review'].values\n",
    "labels = df['positive_review'].values\n",
    "\n",
    "# Load the RoBERTa tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "model = AutoModel.from_pretrained(\"roberta-large\")\n",
    "\n",
    "# Extract features using RoBERTa\n",
    "inputs = np.zeros((len(texts), 1024))\n",
    "for i, text in enumerate(texts):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids).last_hidden_state\n",
    "    inputs[i, :] = last_hidden_states[0, 0, :].numpy()\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(inputs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the classifiers for the voting ensemble\n",
    "svc = SVC(kernel='linear', C=1.0, probability=True)\n",
    "ada = AdaBoostClassifier(n_estimators=100)\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "LogReg_clf = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Define the voting ensemble classifier\n",
    "voting_clf = VotingClassifier(estimators=[('svc', svc), ('ada', ada), ('rf', rf)], voting='soft')\n",
    "\n",
    "# Perform the k-fold cross-validation to tune hyperparameters\n",
    "for train_index, val_index in kf.split(X_train_val):\n",
    "    X_train, X_val = X_train_val[train_index], X_train_val[val_index]\n",
    "    y_train, y_val = y_train_val[train_index], y_train_val[val_index]\n",
    "    \n",
    "    # Train the voting ensemble classifier\n",
    "    voting_clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the classifier on the validation set\n",
    "    val_pred = voting_clf.predict(X_val)\n",
    "    val_acc = accuracy_score(y_val, val_pred)\n",
    "    print(\"Validation accuracy:\", val_acc)\n",
    "    \n",
    "    # Reset the classifiers\n",
    "    svc = SVC(kernel='linear', C=1.0, probability=True)\n",
    "    ada = AdaBoostClassifier(n_estimators=100)\n",
    "    rf = RandomForestClassifier(n_estimators=100)\n",
    "    voting_clf = VotingClassifier(estimators=[('svc', svc), ('ada', ada), ('rf', rf)], voting='soft')\n",
    "\n",
    "# Evaluate the voting ensemble classifier on the test set\n",
    "voting_clf.fit(X_train_val, y_train_val)\n",
    "test_pred = voting_clf.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "print(\"Test accuracy for Voting:\", test_acc)\n",
    "\n",
    "\n",
    "#Bagging Ensemble Method\n",
    "logreg_bagging_model = BaggingClassifier(base_estimator=LogReg_clf, n_estimators=100, random_state=142)\n",
    "#dtree_bagging_model = BaggingClassifier(base_estimator=DTree_clf, n_estimators=100, random_state=142)\n",
    "#random_forest = BaggingClassifier(base_estimator=rf, n_estimators=50, random_state=100)\n",
    "#extra_trees = BaggingClassifier(base_estimator=ETree, n_estimators=50, random_state=100)\n",
    "\n",
    "def bagging_ensemble(bag):\n",
    "    k_folds = KFold(n_splits=20)\n",
    "    results = cross_val_score(bag, X_train_val, y_train_val, cv=k_folds)\n",
    "    print(results.mean())\n",
    "\n",
    "\n",
    "bagging_ensemble(logreg_bagging_model)\n",
    "\n",
    "\n",
    "\n",
    "# get the end time\n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "print('Execution time:', elapsed_time, 'seconds')\n",
    "\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "# Stop tracemalloc\n",
    "tracemalloc.stop()\n",
    "print(\"Current memory usage is\", current / (1024 * 1024), \"MB; Peak was\", peak / (1024 * 1024), \"MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c20477b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
